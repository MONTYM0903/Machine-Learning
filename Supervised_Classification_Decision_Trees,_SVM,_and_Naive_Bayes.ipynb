{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Information Gain, and how is it used in Decision Trees?**\n",
        "\n",
        "ANSWER\n",
        "\n",
        "In simple terms, Information Gain (IG) measures how much “information” or “purity” we gain when we split data based on a particular feature in a decision tree. Decision trees are built by repeatedly splitting data into smaller parts to make the classes as pure as possible. Information Gain helps us decide which feature is the best to split on.\n",
        "\n",
        "Concept\n",
        "\n",
        "Information Gain is based on entropy, which measures impurity or randomness in the data.\n",
        "When we split the dataset, we expect to reduce the randomness.\n",
        "The more reduction in entropy after the split, the higher the Information Gain.\n",
        "\n",
        "The formula is:\n",
        "\n",
        "IG(S,A)=Entropy(S)−v∈values(A) ∑∣S∣∣Sv∣​×Entropy(Sv)\n",
        "\n",
        "Where:\n",
        "\n",
        "S = the whole dataset\n",
        "\n",
        "A = the attribute we split on\n",
        "\n",
        "Sᵥ = subset of data for each value of A\n",
        "\n",
        "\n",
        "Example\n",
        "\n",
        "Suppose we are predicting whether students pass or fail based on whether they study or not.\n",
        "\n",
        "Study\tResult\n",
        "Yes\tPass\n",
        "Yes\tPass\n",
        "No\tFail\n",
        "No\tFail\n",
        "Yes\tPass\n",
        "\n",
        "Before splitting, entropy = 0.97 (mix of pass and fail).\n",
        "If we split on “Study,” the “Yes” group becomes pure (mostly Pass), and “No” group becomes pure (mostly Fail).\n",
        "So, entropy decreases → Information Gain increases.\n",
        "\n",
        "Daigram\n",
        "\n",
        " Root Node\n",
        "              [Pass/Fail]\n",
        "                /     \\\n",
        "           Study=Yes  Study=No\n",
        "           [Pure]      [Pure]\n",
        "\n",
        "This split provides the highest Information Gain.\n",
        "\n",
        "In Decision Trees\n",
        "\n",
        "At every node, the tree checks all features, calculates their Information Gain, and picks the one with the highest gain. This continues until all nodes are pure or other stopping criteria are met.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Information Gain helps the decision tree identify which feature best separates data. It ensures that each split leads to a purer subset, making the model more accurate and interpretable."
      ],
      "metadata": {
        "id": "TNCnL6x1U2iC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the difference between Gini Impurity and Entropy?**\n",
        "\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths, weaknesses, and appropriate use cases.\n",
        "\n",
        "ANSWER:\n",
        "\n",
        "Both Gini Impurity and Entropy are measures of impurity used in decision trees, but they are slightly different in how they calculate impurity and how sensitive they are to class changes.\n",
        "\n",
        "Measure\tFormula\tRange\tBest Split Preference\n",
        "Entropy\t−Σ pᵢ log₂(pᵢ)\t0 to 1\tSlower, more accurate\n",
        "Gini Impurity\t1 − Σ pᵢ²\t0 to 0.5\tFaster, less sensitive\n",
        "\n",
        "**1. Entropy**\n",
        "\n",
        "Entropy comes from information theory and measures the level of disorder or randomness in data.\n",
        "If all samples in a node belong to one class, entropy = 0.\n",
        "If they are evenly split, entropy = 1.\n",
        "\n",
        "Example:\n",
        "If a dataset has 50% Pass and 50% Fail:\n",
        "Entropy = −(0.5 log₂ 0.5 + 0.5 log₂ 0.5) = 1\n",
        "\n",
        "**2. Gini Impurity**\n",
        "\n",
        "Gini Impurity measures how often a randomly chosen element from the dataset would be incorrectly labeled if we randomly labeled it according to the distribution of labels.\n",
        "\n",
        "Example:\n",
        "\n",
        "If the same dataset has 50% Pass and 50% Fail:\n",
        "Gini = 1 − (0.5² + 0.5²) = 0.5\n",
        "\n",
        "Purity Level (lower = better)\n",
        "\n",
        "|        Entropy Curve\n",
        "\n",
        "|      /\n",
        "\n",
        "|     /\n",
        "\n",
        "|----/--- Gini Curve\n",
        "\n",
        "|  /\n",
        "\n",
        "|_/\n",
        "\n",
        "Probability (p)\n",
        "\n",
        "**Comparison**\n",
        "\n",
        "Entropy uses logarithms → more computationally heavy.\n",
        "\n",
        "Gini is simpler and faster to calculate.\n",
        "\n",
        "Both usually lead to similar splits, but Gini tends to isolate the most frequent class first.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Gini Impurity and Entropy serve the same purpose: measuring impurity in nodes. Gini is simpler and faster, while Entropy gives a more theoretical information-based measure. In practice, both give similar results, and the choice depends on speed versus interpretability."
      ],
      "metadata": {
        "id": "XuPMa64tWFBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is Pre-Pruning in Decision Trees?**\n",
        "\n",
        "ANSWER\n",
        "\n",
        "When we build a decision tree, it can easily become too complex and start memorizing the training data instead of learning patterns. This problem is called overfitting. To avoid it, we use pruning, which means cutting the tree shorter.\n",
        "Pre-pruning means stopping the tree from growing too large while it’s being built.\n",
        "\n",
        "Concept\n",
        "\n",
        "Instead of allowing the tree to grow fully and then cutting it, pre-pruning stops splitting early based on certain conditions such as:\n",
        "\n",
        "Maximum tree depth\n",
        "\n",
        "Minimum number of samples per leaf\n",
        "\n",
        "Minimum information gain required for a split\n",
        "\n",
        "Example\n",
        "\n",
        "Suppose we have a tree that perfectly classifies all training examples. However, when we test it on new data, the accuracy drops sharply. That means it overfitted.\n",
        "If we had used pre-pruning, we could have stopped it at a smaller, more general level.\n",
        "\n",
        "Diagram\n",
        "\n",
        "Without Pre-Pruning          With Pre-Pruning\n",
        "      Root                        Root\n",
        "     /   \\                       /   \\\n",
        "   Split Split                 Decision\n",
        " Too Deep Tree             Simpler General Tree\n",
        "\n",
        "**Pre-Pruning Parameters in Python**\n",
        "\n",
        "max_depth: limits how deep the tree grows\n",
        "\n",
        "min_samples_split: minimum samples to split a node\n",
        "\n",
        "min_samples_leaf: minimum samples per leaf node\n",
        "\n",
        "max_leaf_nodes: maximum leaves allowed\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Pre-pruning prevents decision trees from growing unnecessarily complex, improving generalization. It’s a proactive approach that controls tree size during training rather than fixing overfitting afterward."
      ],
      "metadata": {
        "id": "1YAnjCJwXQkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).**\n",
        "\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n"
      ],
      "metadata": {
        "id": "hLpExyL5XjNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##ANSWER\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train Decision Tree using Gini\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Display Feature Importances\n",
        "for name, importance in zip(data.feature_names, model.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBcezEsRXqSD",
        "outputId": "da769086-1973-4722-8835-3f9c80b0f9c3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sepal length (cm): 0.0133\n",
            "sepal width (cm): 0.0000\n",
            "petal length (cm): 0.5641\n",
            "petal width (cm): 0.4226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "\n",
        "The criterion='gini' tells the model to use Gini Impurity for splitting.\n",
        "\n",
        "After fitting, .feature_importances_ shows which features contributed most.\n",
        "\n",
        "For the Iris dataset, usually petal length and petal width are most important.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Using Gini Impurity, the decision tree identifies the most informative features efficiently. This method helps prioritize variables that best separate the classes."
      ],
      "metadata": {
        "id": "IkcL92PfXuC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What is a Support Vector Machine (SVM)?**\n",
        "\n",
        "ANSWER\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the best boundary (hyperplane) that separates different classes in the data.\n",
        "\n",
        "Concept\n",
        "\n",
        "SVM tries to find a line (in 2D), plane (in 3D), or hyperplane (in higher dimensions) that maximizes the margin, which is the distance between the boundary and the nearest data points from each class (called support vectors).\n",
        "\n",
        "Diagram\n",
        "\n",
        "Class A (●)      | Margin |       Class B (○)\n",
        "● ● ● ● ● ●  -------|-------- ○ ○ ○ ○ ○\n",
        "      Support Vectors\n",
        "\n",
        "Example\n",
        "\n",
        "Suppose we want to classify emails as spam or not spam. Each email is represented by numerical features (like word count or keyword frequency).\n",
        "SVM finds the best line or surface that divides spam emails from non-spam emails with maximum margin.\n",
        "\n",
        "Mathematical Idea\n",
        "\n",
        "The goal is to maximize:\n",
        "\n",
        "Margin = 2∣∣𝑤∣∣\n",
        "\n",
        "where w is the weight vector.\n",
        "\n",
        "Advantages\n",
        "\n",
        "Works well for high-dimensional data\n",
        "Effective even when the number of features > samples\n",
        "Uses different kernels to handle non-linear data\n",
        "\n",
        "Conclusion\n",
        "\n",
        "SVM is a powerful algorithm that aims to find the most optimal decision boundary between classes. Its ability to handle both linear and non-linear data makes it one of the most reliable classifiers in data analytics."
      ],
      "metadata": {
        "id": "MJ3ylecqX5fD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: What is the Kernel Trick in SVM?**\n",
        "\n",
        "ANSWER\n",
        "\n",
        "Not all data can be separated with a straight line. The Kernel Trick allows SVM to classify data that is not linearly separable by transforming it into a higher-dimensional space where it becomes separable.\n",
        "\n",
        "Example\n",
        "\n",
        "Imagine data shaped like two concentric circles.\n",
        "In 2D, it’s impossible to draw a straight line to separate them.\n",
        "But if we project it into 3D space, the circles become separable by a plane.\n",
        "\n",
        "Diagram:\n",
        "\n",
        "2D:            3D (after kernel)\n",
        " ○ ○ ○          ↑\n",
        "○     ○         | Plane separates easily\n",
        " ○ ○ ○\n",
        "\n",
        "\n",
        "How Kernel Trick Works\n",
        "\n",
        "SVM doesn’t actually compute the transformation directly. It uses a mathematical function (kernel) to compute inner products in higher dimensions without explicitly transforming data.\n",
        "\n",
        "Common Kernels\n",
        "\n",
        "Linear Kernel → works for linearly separable data\n",
        "\n",
        "K(x,y)=xTy\n",
        "\n",
        "Polynomial Kernel → good for curved boundaries\n",
        "\n",
        "K(x,y)=(xTy+c)d\n",
        "\n",
        "RBF (Radial Basis Function) → best for complex, non-linear boundaries\n",
        "\n",
        "   K(x,y)=e−γ∣∣x−y∣∣2\n",
        "\n",
        "Conclusion\n",
        "\n",
        "The Kernel Trick makes SVM extremely flexible. It allows linear algorithms to solve non-linear problems efficiently without increasing computational cost."
      ],
      "metadata": {
        "id": "PDqeZsuoYM6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.**\n",
        "\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting on\n",
        "the same dataset.\n"
      ],
      "metadata": {
        "id": "Q8AijEUJZAua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.data, data.target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Linear Kernel\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, linear_svm.predict(X_test))\n",
        "\n",
        "# RBF Kernel\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, rbf_svm.predict(X_test))\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", linear_acc)\n",
        "print(\"RBF Kernel Accuracy:\", rbf_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wJdCrBrZEfD",
        "outputId": "385fbc9e-ffda-46c5-c1b0-61fe5930a566"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9814814814814815\n",
            "RBF Kernel Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "\n",
        "- The linear kernel works well if the data is linearly separable.\n",
        "- The RBF kernel is more flexible and usually gives better accuracy for complex data.\n",
        "- You can compare both results; often, RBF performs slightly better."
      ],
      "metadata": {
        "id": "jB6W5T5AZHC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: What is the Naïve Bayes Classifier, and Why is it Called “Naïve”?**\n",
        "\n",
        "ANSWER\n",
        "\n",
        "Naïve Bayes is a classification algorithm based on Bayes’ Theorem. It assumes that all features are independent of each other, which is rarely true in real life — that’s why it’s called “naïve.”\n",
        "\n",
        "Bayes’ Theorem\n",
        "       P(A∣B)=P(B)P(B∣A)×P(A)\n",
        "\t​\n",
        "In classification terms:\n",
        "\n",
        "P(Class∣Features)∝P(Features∣Class)×P(Class)\n",
        "\n",
        "Example\n",
        "\n",
        "Suppose we want to predict if an email is spam based on two words: “offer” and “discount.”\n",
        "\n",
        "Naïve Bayes assumes the occurrence of these words is independent, even though in reality they might co-occur often.\n",
        "\n",
        "Diagram\n",
        "\n",
        "Word Features → [offer, buy, click]\n",
        "\n",
        "          ↓\n",
        "\n",
        "   Naïve Bayes Model\n",
        "\n",
        "          ↓\n",
        "\n",
        "     Predict: Spam / Not Spam\n",
        "\n",
        "\n",
        "Advantages\n",
        "\n",
        "Fast and simple\n",
        "\n",
        "Works well with small datasets\n",
        "\n",
        "Great for text and email classification\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Naïve Bayes is simple but effective, especially in text-related tasks. The “naïve” independence assumption simplifies computation, making it both efficient and practical."
      ],
      "metadata": {
        "id": "BY3bUlNqZofO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes ?**\n",
        "\n",
        "\n",
        "ANSWER\n",
        "\n",
        "Type\tData Type\tUse Case\tDistribution\n",
        "Gaussian NB\tContinuous\tSensor data, real values\tNormal Distribution\n",
        "Multinomial NB\tDiscrete counts\tWord frequencies, text\tMultinomial\n",
        "Bernoulli NB\tBinary features\tPresence/absence\tBernoulli\n",
        "\n",
        "1. Gaussian Naïve Bayes\n",
        "\n",
        "Used when features are continuous (e.g., height, temperature).\n",
        "It assumes data follows a normal distribution.\n",
        "\n",
        "P(xi\t​∣y)=2πσy2-1e2σy2\t(xi−μy)2\n",
        "\n",
        "\n",
        "2. Multinomial Naïve Bayes\n",
        "\n",
        "Used for discrete counts, such as word frequencies in text.\n",
        "Common in document classification.\n",
        "\n",
        "3. Bernoulli Naïve Bayes\n",
        "\n",
        "Used when features are binary (0 or 1).\n",
        "Example: if a word is present (1) or absent (0) in a document.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Each Naïve Bayes variant is suited to specific data types. Choosing the right one ensures better model performance and interpretability."
      ],
      "metadata": {
        "id": "WcA5HM2laHe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Breast Cancer Dataset**\n",
        "\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from sklearn.datasets.\n"
      ],
      "metadata": {
        "id": "1WcWgwEUbEf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.data, data.target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gaussian Naive Bayes\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUewrCV6bKuY",
        "outputId": "c0cfc000-ce69-4a4e-b340-4f2883eeb4a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9415204678362573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "\n",
        "The Breast Cancer dataset contains real-valued medical measurements.\n",
        "\n",
        "GaussianNB is ideal because it assumes a normal distribution.\n",
        "\n",
        "The model usually achieves around 94–96% accuracy, showing its reliability.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Gaussian Naïve Bayes is efficient for continuous medical datasets like Breast Cancer. Its speed and accuracy make it an excellent baseline model in data analysis projects."
      ],
      "metadata": {
        "id": "p2X-OUDZbMY8"
      }
    }
  ]
}